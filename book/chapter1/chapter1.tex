\chapter{Modeling and simulation}
\section{Statistical models}

A central concept to this course is that of a {\dfn model}. %Before doing any mathematics, let's reflect on the notion of a model in the abstract.
%\begin{exercise}
%What does it mean to model something?
%\end{exercise}
Broadly speaking, models are simplified representations of the world\footnote{Model is sometimes used interchangeably with {\dfn theory}, although I usually think of models as being smaller in scope.} There are many ways to represent models, but in science (and life), we often use {\dfn mathematical models}.
For example, you might be familiar with Newton's equation:
\begin{equation}\label{eq:fma}
F = ma
\end{equation}
which related the force ($F$) acceleration ($a$) and mass ($m$) of a particle. This a mathematical model of the motion of a  (non-relativistic) particle in a force field. While it isn't always true, it holds for such a wide range of applications that we call it a \emph{fundamental} law of nature, and it turns out to be extremely powerful. We can combine Newton's equation with other fundamental lows build models of more complicated systems involving many particles. For example, to build a model of planetary motion, we can combine Newton's equation with his other law of gravity, which states that the gravitation force between two objects of masses $m_1$ and $m_2$ a distance $r$ from each other is
\begin{equation}
F = G\frac{m_1m_2}{r^2}
\end{equation}
where $G$ is a constant.
We refer to these types of models -- that is, those which are built upon fundamental laws of nature -- as {\dfn mechanistic} models.
%Other mechanistic models are the equation of the number of chemicals in a well-mixed vessel (based on the law of mass action) and the equation for the

In statistics, we are often interested in problems where nothing remotely close to a fundamental laws exist (yet). Instead, they are based directly on observations (data) or our intuitions. We will call these {\dfn phenomenological models}. Such models still be quite useful provided we are aware of their limitations. Of course, there is not sharp distinction between mechanistic and phenomenological models, but the distinction is helpful nonetheless. 


Often our models cannot make exact predictions about the value of a variable (this is true in both mechanistic and phenomenological models, but especially the latter). Instead, they only tell us the probability that a variable has a certain value, or falls within a range of values. 

For example, if we were to construct a model of the $(y)$ of the height a randomly selected pine tree in New Hampshire as a function of its age $(x)$, we might begin by searching a relationship of the form
\begin{equation}
y = f(x)
\end{equation}
Such a model might be relevant for conservation efforts, since it would be important to understand how trees develop over time and influence their surrounding environment. 
However, if we take a {\dfn sample} of the population, meaning we go out and measure the height of some trees for which we know the age, we will quickly find that trees of the same age can have different heights.   This variation will be as result of many variables which are not in our model, e.g. the surrounding, the specific subspecies of pine, genetic variation within a subspecies to name just a few. In principle, we could construct a more complex model which includes, say, the DNA sequence of the tree ($g$):
\begin{equation}
y = f(x,g).
\end{equation}
This model would presumably have less variation. That is, if we collect a sample of trees for which we know the height and DNA sequence, we would find that the variation between trees with the same DNA and height is less than the variation between trees with only the same height. Yet variation will remain unless we include all the variables which effect tree height. However, it is not so useful to include these variables, since we can't actually measure most of them and many will have only a small influence on the tree height. One approach to dealing with these ``hidden" sources of variation is to define a {\dfn statistical model}, where $y$ is not the same for each tree of the same height, but is instead a {\dfn random variable}. On type of a statistical model is a regression model
\begin{equation}\label{eq:reg}
y = f(x) + \epsilon
\end{equation}
where $\epsilon$ is a random variable (usually one that is zero, on average). Models of this sort are the topic of this course. 

%\begin{exercise}
%When we write down a model, statistical or otherwise, we make assumptions. What are some of the assumptions we have made by writing down Equation \eqref{eq:reg}? Hint: think about other ways in which you might compute a random term, $\epsilon$, with a function like $f$.
%\end{exercise}


\section{Random variables and distributions}

The rigorous mathematical theory for random variables is very useful, but requires certain machinery which is beyond the scope of these notes. Fortunately, we go a long way without such formalism. For our purposes, a random variable can be understood as a variable which we cannot predict prior to an observation, regardless of how much information we have.
We can define the space of  {\dfn outcomes} as all the possible values that a random variable may take on. The outcomes for the roll of a dice are $1,2,\dots,6$ for the dice, or positive numbers for the height of a tree.  Usually the outcomes are numbers, even if we use a number to represent a non-numerical quantity (e.g. someone's gender). In probability theory, one distinguishes between outcomes and {\dfn events} -- the latter are subsets of outcomes. For example, we might refer to the event that the roll of a die is grater than $2$. It's good to be aware of these definitions, but you don't need to memorize them.



We can describe a characterize a random variable using  a {\dfn probability distribution}, which maps a set of possible events to real numbers between 0 and 1.   For example, suppose we ask a random student in the college whether they were born in the US. A probability distribution $P(Y)$ which \emph{models} their answer is the {\dfn Bernoulli distribution},
\begin{equation}
P(Y) = \left\{ \begin{array}{cc}
q & Y={\rm YES}\\
1-q & Y={\rm NO}
\end{array}
 \right.
\end{equation}
where $q$ is the fraction of students in the college who were born in the US. More compactly, we can represent YES with $1$ and NO with $0$, and write
\begin{equation}
P(Y) = q^Y(1-q)^{1-Y}.
\end{equation}
  We say that $q$ is a {\dfn parameter} in our model because regardless of it's value our model is still a Bernoulli distribution.  It is very important that the sum of $P(Y)$ over all possible outcomes is $1$ -- this is simply saying that we are certain one of the outcomes will happen. We will use $P(1)$ to mean ``the probability that $Y=1$", or, in there is some ambiguity in which variable we are referring to, we might write $P(Y=1)$. 
 
 In this context, $q$ has a clear interpretation: it is the fraction of students in the college who were born in the US. The Bernoulli distribution is our default model for any variable that can take two possible outcomes, usually abstracted as $0$ or $1$.  In order to state that a Bernoulli distribution is a model for some random variable $Y$, we write
\begin{equation}
Y \sim {\rm Bernoulli}(q).
\end{equation}
We might also say ``$Y$ \emph{follows} as Bernoulli distribution" or ``$Y$ is a Bernoulli random variable".
More generally, we say that a variable in a model follows a given distribution by writing
\begin{equation}
{\rm Variable} \sim {\rm Distribution}({\rm parameters}).
\end{equation}
We will sometimes use $\theta$ to denote the parameters. 


Turning back to the example of our survey, let's suppose we don't have information about every students in the college. Rather, a survey of five students from this class is conducted, finding $4$ yeses and $1$ no. What is our best prediction of the total fraction of students in the college who answered YES? What assumption do we make when we answer this question? The process of answering this question is {\dfn statistical inference}. More generally, we use statistical inference to make predictions about things we don't observe based one what we do observe (data). 




%\begin{exercise}
%Suppose we are counting the number  $\{1,2,3,4,5,6,7,8,9,12\}$
%\end{exercise}
\section{Python as a tool for statistical modeling}
When we generate samples using a computer we call them {\dfn simulations}. We will use python to perform simulations, and it is therefore important to have a basic understanding of the python language. It is assumed that you will go through the separate python tutorial notebook. For convenience, we will cover some basic tasks in this \href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=_c4br6SCUtUy}{Notebook}

%First, we will assign variables and perform arithmetic. 

%These operations are pretty intuitive: 
%\begin{Verbatim}
%x = 1
%y = 2
%z = x + y
%print(z)
%z = x*y 
%print(z)
%\end{Verbatim}
%We will work heavily with vectors, or lists of numbers in this course. 
%\begin{Verbatim}
%a = [1,2,3] # make empty list
%a.append(5) # add 5 to the end of the list
%print(a) # print the list
%print(a[2])	# index the list
%print(a[0:2])	
%print(a[a>2]) # obtain a subset of the list
%# easy way to make an array
%a = range(5)
%print(a[0])
%\end{Verbatim}
%We will work with a package called numpy, which provides a wrapper for the python list affording us some additional functionality. 
%\begin{Verbatim}
%import numpy as np # import the numpy package
%a = np.zeros(5)
%a = np.array([0,0,0,0,0])
%# advantage of numpy arrays:
%a = np.array([1,2,3])
%b = np.array([5,3,2])
%a + b
%# get the length of an array 
%len(a)
%# generate array 
%a = np.linspace(0,1,100)
%print(a)
%\end{Verbatim}
%Loops allow us to repeat an operation, or carry out an operation over some range of values
%\begin{Verbatim}
%for k in range(5):
%	print(k)	
%\end{Verbatim}
%The same thing can be achieved with a while loop 
%\begin{Verbatim}
%k = 0
%while k<5: 
%	print(k)
%	k = k+1
%\end{Verbatim}
%
%In many instances, it is helpful to put code we will reuse in a function. 
%\begin{Verbatim}
%def test_func(z)
%	k = 0
%	while k<z: 
%		print(k)
%		k = k+1
% 	return k
%a = test_func(k)
%\end{Verbatim}


\begin{exercise}
\href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=_c4br6SCUtUy}{Working with for loops}
\end{exercise}
%\begin{solution}
%Making only a few changes to the existing code and putting it in a function 
%\begin{Verbatim}
%def print_triangle(n)
%    for i in range(n):
%      for j in range(i)
%        print(i)
%        print(",")
%      print(i)
%      println()
%\end{Verbatim}
%\end{solution}
%\begin{exercise}
%Determine what the following lines for code will output
%\begin{enumerate}
%\item What will be the value of $z$ after the following line of code is run?
%\begin{Verbatim}
%z = 1
%for k in range(3):
%  z = z+k
%\end{Verbatim}
%\item What will the following code print?
%\begin{Verbatim}
%x = 11
%k = 1
%while x>2:
%  x = x-2
%  print(k)
%  k = k+1
%\end{Verbatim}
%
%\item Consider the code
%\begin{Verbatim}
%for i in range(5):
%  for j in range(i)
%    print(i)
%    print(",")
%  println()
%\end{Verbatim}
%modify this code to print
%\begin{Verbatim}
%1 *
%1,2 *
%1,2,3 *
%1,2,3,4 *
%\end{Verbatim}
%\end{enumerate}
%\end{exercise}


\subsection{Simulations}
 Here, we will focus on tools relevant for statistics. In Python, we can simulate random variables using the \verb|numpy| library:
\begin{Verbatim}
import numpy as np
q = 0.5
y = np.random.choice(range(2),p=[q,1-q])
 \end{Verbatim}
 We can generate multiple samples using a for loop
\begin{Verbatim}
n_samples = 100
y = np.zeros(n_samples) # makes an empty list (i.e. array) of n_samples zeros.
for k in np.range(n_samples):
  y[k] = np.random.choice(range(2),p=[q,1-q])
 \end{Verbatim}
A simpler way of doing this is
\begin{Verbatim}
y = np.random.choice(range(2),n_samples,p=[q,1-q])
 \end{Verbatim}


  The more general form of this command is
\begin{Verbatim}
y = np.random.choice(range(k),n_samples,p=[q_1,q_2,...,q_k])
 \end{Verbatim}
 where $q_1 + \cdots q_k = 1$. This will generate a sample from



 \begin{exercise}
 \href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=_c4br6SCUtUy}{Building a probability model}
\end{exercise}
%\begin{solution}
%For the fair die we have
%\begin{Verbatim}
%y = np.random.choice(range(6),n_samples,p=[1/6,1/6,1/6,1/6,1/6,1/6])
%\end{Verbatim}
%For the other one we have
%\begin{Verbatim}
%y = np.random.choice(range(6),n_samples,p=[0,2/6,1/6,1/6,1/6,1/6])
%\end{Verbatim}
%\end{solution}

 We can also generate simulations of more complex random variables using simple ones.  In this case, it is useful to define a function in Python which generates samples of our new random variable. For, example:
 \begin{example}
 \href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=xCU9OVTijAmC&line=2&uniqifier=1}{Writing a function to run simulations of coin flips.}
\end{example}
%\begin{solution}
%\,
%\begin{Verbatim}
%def flip_until_two():
%  num_heads = 0
%  total_flips = 0
%  while num_heads <2:
%    y = np.random.choice([0,1])
%    if y == 0:
%      num_heads = 0
%    else:
%      num_heads = num_heads + 1
%    total_flips = total_flips + 1
%  return total_flips
% \end{Verbatim}
% \end{solution}



\begin{exercise}
 \href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=YfinIy4lsXnH}{Modifying existing code}
\end{exercise}
%\begin{solution}
%We need to change (1) the distribution we sample $y$ from and (2) the condition to stop. 
%\begin{Verbatim}
%def dice_until_two():
%  num_ones = 0
%  total_rolls = 0
%  while num_ones <2:
%    y = np.random.choice([1,2,3,4,5,6])
%    if y == 0:
%      num_ones = 0
%    else:
%      num_ones = num_ones + 1
%    total_rolls = total_rolls + 1
%  return total_rolls
% \end{Verbatim}
%\end{solution}

\subsection{Visualization}

 An important tool for visualizing samples is a histogram. In python, we would write:
\begin{Verbatim}
plt.hist(samples,100,density=true)
 \end{Verbatim}
 The histogram shows us the frequency of different outcomes. Histograms are discussed \href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=UkD_oWqXUq_k}{here}
 
 \subsection{Working with tabular data}
 
 Frequently, we will work with data in tabular form. We can do this using Numpy (hopefully you read about this in the python tutorial), e.g. 
 \begin{Verbatim}
 # imagine we have an array of times and corresponding temperature measurements:
times = np.array([1,2,3,4,5])
temps = np.array([72,71,75,75,73])
# we can make a 2d numpy array 
data = np.transpose(np.array([times,temps]))
data
 \end{Verbatim}
 The pandas package in python provides some additional functionality:
 \begin{Verbatim}
# the pandas library provides a convenient way organize this data
import pandas as pd
df = pd.DataFrame(data,columns = ["time","tempature"])
 \end{Verbatim}

Examples from class can be found here \href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=UkD_oWqXUq_k}{here}.


\subsection{Monte Carlo}
Often, we run many simulations of a model in order to say something about the distribution without performing any analytical calculations. We call these {\dfn Monte Carlo} simulations. Monte Carlo simulations make use of the fact that we can always conceptualize probabilities as fraction of things. That is, if we have $n$ samples of a variable $Y$ and we want to estimate $P(Y=y)$, then we can count the number for which $Y=y$ -- we denote this as $n(Y=y)$, and divide by the total number: $P(Y=y) \approx n(Y=y)/n$. 
\begin{example}
\href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=n7vN0l4PWbbX}{Running Monte Carlo simulations}
\end{example}
Questions concerning how many samples we need to generate to obtain meaningful estimates from Monte Carlo simulations will be addressed later on. 
%\begin{solution}
%We will make a loop which performs the coin flipping many times and save the number of flips. 
%\begin{Verbatim}
%n_monte = 1000
%Js = np.zeros(n_monte)
%for k in range(n_monte):
%  Js[k] = flip_until_two()
%len(Js[Js>5])/len(Js)
%plt.hist(Js)
% \end{Verbatim}
%\end{solution}







\subsection{Means,  variances, etc.}
There are ways in which we summarize attributes of random variables.
If we have many samples $Y_1,Y_2,\dots,Y_n$ of a random variable $Y$ (e.g. answers to a survey question), the {\dfn sample mean} is defined as
\begin{equation}
\bar{Y} = \frac{1}{n}\sum_{i}Y_i
\end{equation}
 Often it is useful to quantify the deviations from the mean. Suppose each $Y_i$ can take on outcomes $Y = 1,2,3,\dots,m$. If $n$ is large, then the fraction of samples for which $Y_1= y$ will be $P(Y_1=y)$, thus the sample mean converges to the true mean: 
 \begin{equation}
\bar{y} = \frac{1}{n} \sum_{y=1}^m y n_i=\sum_{y=1}^m y \frac{n_i }{n}\approx  \sum_{y=1}^m y P(Y=y)
\end{equation}
Sometimes we write $P(Y_i = y_i)$ and sometimes we write 
The expression on the right is the definition of the mean, or {\dfn expectation}, often denoted $\E[Y]$.



 For this, we have the {\dfn sample variance}
\begin{equation}
\sigma^2 = \frac{1}{n-1}\sum_{i}(y_i-\bar{y})^2.
\end{equation}
We will see why this makes sense as a measure of how spread our a distribution is later on when we talk about inference. For large $n$, this converges to the square root of the variance
\begin{equation}
{\rm Var}(Y) = \sum (y_i - \E[Y])^2 P(Y_i=y_i).
\end{equation}

The sample standard deviation is:
\begin{equation}
\sigma = \sqrt{\frac{1}{n-1}\sum_{i}(y_i-\bar{y})^2}.
\end{equation}





%\begin{example}
%Write a python function in python that computes the mean and standard deviation
%\end{example}


In python, functions for implementing the mean and standard deviation are follows:
\begin{Verbatim}
np.mean(y)
np.std(y)
\end{Verbatim}
By default, the standard deviation function divides the sum by the number of samples, we can fix this 
\begin{Verbatim}
np.std(y,ddof=1)
\end{Verbatim}

%\begin{example} Use Monte Carlo simulations to study how the mean and variance of the number of flips it takes to get two
%
%\end{example}


\begin{example}
\href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=36vlB9r1Wts5}{Verifying an analytical formula with simulations}
\end{example}

%\begin{solution}
%The following code will estimate the mean and standard deviations for various values of $q$:
%\begin{Verbatim}
%def mean_bern(q):
%  sample = np.random.choice([0,1],500,p=[1-q,q])
%  return np.mean(sample)
%
%# generates one hundred evenly spaced numbers between 0 and 1
%q_range = np.linspace(0,1,100)
%means = np.zeros(len(q_range))
%for j in range(len(q_range)):
%  means[j]= mean_std_bern(q_range[j])
%\end{Verbatim}
%Here is a plot of the mean compared to the prediction
%\begin{Verbatim}
%fig,ax = plt.subplots(figsize=(5,2))
%ax.plot(q_range,means)
%ax.plot(q_range,q_range,"k--")
%\end{Verbatim}
%
%\end{solution}


\begin{exercise}
\href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=69BY8z2IRXnU&line=6&uniqifier=1}{Verifying an analytical formula with simulations}
\end{exercise}
%\begin{solution}
%We only need to change the mean to the standard deviation
%\begin{Verbatim}
%def std_bern(q):
%  sample = np.random.choice([0,1],500,p=[1-q,q])
%  return np.std(sample)
%
%# generates one hundred evenly spaced numbers between 0 and 1
%q_range = np.linspace(0,1,100)
%means = np.zeros(len(q_range))
%for j in range(len(q_range)):
%  stds[j]= std_bern(q_range[j])
%\end{Verbatim}
%\end{solution}




We can always calculate the mean and standard deviation and mean of a sample regardless of the distribution it has been drawn from. However, we need to be careful, as the results may not be so meaningful. For example, the mean of a Bernoulli random variable is $q$, but (unless $q=0$ or $q=1$), the variable will never actually take on this value. For example, it might be more meaningful to think of the {\dfn mode}, which is the value that occurs most frequently. 






\section{Joint probabilities, Independence} We introduce, very briefly, the concepts of independence and conditioning. Just as we have considered single random variables, we can consider multiple random variables within the same model. Suppose we have two Bernoulli random variables $Y_A$ and $Y_B$ which model whether a person has mutations at two different locations on their genome. In this case, we need a model of both variables together:
\begin{equation}
\Prob(Y_A,Y_B) = \left\{ \begin{array}{cc}
q_{00} & \text{ if }Y_A=0 \text{ and } Y_B = 0\\
q_{01} & \text{ if }Y_A=0 \text{ and } Y_B = 1\\
q_{10} & \text{ if }Y_A=1 \text{ and } Y_B = 0\\
q_{11} & \text{ if }Y_A=1 \text{ and } Y_B = 1\\
\end{array}
 \right.
\end{equation}



%\begin{solution}
%There are $4$ outcomes, so we can identify each with a number as follows:
%\begin{equation}
%(0,0) \to 1,\quad (0,1) \to 2,\quad (1,0) \to 3,\quad (1,1) \to 4
%\end{equation}
%We then use the random choice function
%\begin{Verbatim}
%np.random.choice([1,2,3,4],p=[1/8,1/8,1/4,1/2])
%\end{Verbatim}
%\end{solution}



The probability distribution $P(Y_A,Y_B)$ tells us the probabilities for observing \emph{both} variables together, e.g. observing a person with both mutations. It does not directly tell us the probabilities of observing e.g. someone with only one mutation. This can be obtained via marginalization; that is, summing over the other variable:
\begin{equation}
P(Y_A)  = \sum_{y}P(Y_A,y) = P(Y_A,Y_B = 0)  +P(Y_A,Y_A = 1)
\end{equation}
where in the general the sum is taken over all possible outcomes for the second variable. $\Prob(Y_1)$ is defined similarly. 
For example, 
\begin{equation}
P(Y_A = 1) = q_{10} + q_{11}.
\end{equation}
This means that 
\begin{equation}
Y_A \sim {\rm Bernoulli}(q_{10}+q_{11}).
\end{equation}
This is the distribution of $Y_A$ absent any knowledge of $Y_B$. What if we are interested in the chance that someone has a mutation in gene $A$ and we know they do not have a mutation in gene $B$?  In this case, we introduce the {\dfn conditional probability} $P(Y_A=1|Y_B=0)$. This is defined as the chance that gene A has a mutation in a person if we know there is no mutation at gene B. If we want to think about this in terms of population averages, it is the fraction of mutations in gene $A$ among only those people without mutations in gene gene $B$. 


How do we calculate this? Using $N$ to denote the number of individuals in a population with a given gene configuration,  
\begin{align}
P(Y_A=1|Y_B=0) &= \frac{N(Y_A = 1,Y_A = 0)}{N(Y_B=0)} = \frac{N(Y_A = 1,Y_A = 0)/n}{N(Y_B=0)/n} \\
&= \frac{P(Y_A = 1,Y_B = 0)}{P(Y_B = 0)}
\end{align}
This is a specific instance of Bayes' formula: 
\begin{equation}\label{eq:bayes}
P(Y|X) = \frac{P(Y,X)}{P(X)}
\end{equation}
Two variables are said to be {\dfn independent} if $P(Y|X)  = P(Y)$.  


Can you see why $X$ being independent of $Y$ implies $Y$ is independent of $X$?
Equation \eqref{eq:bayes} is also true for events, for example, we will encounter things like 
\begin{equation}\label{eq:bayes}
P(Y>z|X) = \frac{P(Y>z,X)}{P(X)}
\end{equation}
For our purposes, it is important to understand the process of conditioning with data. 

\begin{example}
\href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=roga82kQRaau}{Showing independence}
\end{example}

\begin{example}
\href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=roga82kQRaau}{Conditional averages}
\end{example}
%\begin{solution} 
%First we generate some data
%\begin{Verbatim}
%y = np.random.choice([1,2,3,4],p=[1/8,1/8,1/4,1/2])
%\end{Verbatim}
%We can then obtain the sample conditioned on $Y_2$ having the mutation: 
%\begin{Verbatim}
%y[y ==1 or y==3]
%\end{Verbatim}
%The conditional probability would be 
%\begin{Verbatim}
%len(y_sub[y==1])/len(y_sub)
%\end{Verbatim}
%\end{solution}




%
%\begin{example}
%In this case, we have
%\begin{equation}\label{eq:ex_bayes}
% \frac{P(Y_1=2,X_2=0)}{P(Y_2=0)} = \frac{q_{10}}{q_{10}+q_{01}}
%\end{equation}
%Use Monte Carlo simulations to confirm Equation  \eqref{eq:ex_bayes} for the values given in exercise ??.
%\end{example}
%
%\begin{solution}
%This is achieved by the following code:
%\begin{Verbatim}
%y = np.random.choice([1,2,3,4],1000,p=[1/8,1/8,1/4,1/2])
%y_sub = y[y ==1 or y==3]
%len(y_sub[y==1])/len(y_sub)
%\end{Verbatim}
%\end{solution}

\begin{exercise}
\href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=HT5mXESCXWYx}{Estimating conditional probability of dice}
\end{exercise}

\begin{exercise}
\href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=HT5mXESCXWYx}{Conditioning in gene model}
\end{exercise}

\section{Additional exercises}

\begin{exercise}
\href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=vogWBcGHaZDM&line=3&uniqifier=1}{Working with homocide data}
\end{exercise}

\begin{exercise}
\href{https://colab.research.google.com/drive/1Gs-gSsUP1hHVwhrbwvWzLVm1ulcLJKRI#scrollTo=TnLORrmyBn6q&line=22&uniqifier=1}{Simulating covid}
\end{exercise}
%\begin{solution}
%This is achieved by the following code:
%\begin{Verbatim}
%y = np.random.choice([1,2,3,4,5,6],1000,p=[1/6,1/6,1/6,1/6,1/6,1/6])
%y_sub = y[y >2]
%len(y_sub[y==3])/len(y_sub)
%\end{Verbatim}
%\end{solution}




%\begin{exercise}
%In this model you will combine what you've learned to build a model for student birthdays.
%\begin{enumerate}
%\item Construct a model for a statistical model for a students birthday. That is, you should treat a students birthday $Y$ (a number between $1$ and $366$ (to account for leap years).
%\item Write python code which generates samples from this model.
%\item Use Monte Carlo simulations to determine the chance that two students share the same birthday.
%\end{enumerate}
%\end{exercise}





